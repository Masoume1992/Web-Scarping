import googleapiclient.discovery
import googleapiclient.http
import pandas as pd
from isodate import parse_duration
import time
start_time=time.time()

api_key = 'AIzaSyDmA510Zt8nC7rF8tefR9R-tX5Xzs2vEuk' # WebScraping 16
youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)

channel_names= ['The Nutri Gurl',
                'Hayls World'
                ]

# Funtion 1: Fetch Channel IDs
def get_channel_ids(channel_names):

    channel_ids=[]

    for channel_name in channel_names:
        request = youtube.search().list(
        part='id',q=channel_name,type='channel',maxResults=1)

        search_response=request.execute()

        if 'items' in search_response:
            channel_id = search_response['items'][0]['id']['channelId']
            channel_ids.append(channel_id)
        else:
            print(f"Channel '{channel_name}' not found.")
            continue

    return channel_ids


# 2-Fetch Channel Info containing Playlist IDs
def get_channel_info(channel_ids):

    all_channels_data = []

    response = youtube.channels().list(
        part='id,snippet,statistics,brandingSettings,contentDetails,status,topicDetails',
        id=','.join(channel_ids)).execute()

    for i in range(len(response['items'])):
        data={
            'Channel_ID': response['items'][i]['id'],
            'Channel_Name': response['items'][i]['snippet']['title'],
            'Subscriber_Count': response['items'][i]['statistics']['subscriberCount'],
            'Channel_Description': response['items'][i]['snippet']['description'],
            'Joined_Date': response['items'][i]['snippet']['publishedAt'],
            'Total_Views': response['items'][i]['statistics']['viewCount'],
            'Location': response['items'][i]['snippet']['country'],
            'Keywords' : response['items'][i]['brandingSettings']['channel'].get('keywords', []),
            'Number_of_Videos': int(response['items'][i]['statistics']['videoCount']),
            'Playlist_ID': response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],
            'Privacy_Status':response['items'][i]['status']['privacyStatus'],
            'Channel_Topic_Categories': response['items'][i]['topicDetails']['topicCategories'],#A list of Wikipedia URLs that describe the channel's content.
            'Channel_Moderate_Comments':response['items'][i]['brandingSettings']['channel'].get('moderateComments','Flase'),#This setting determines whether user-submitted comments left on the channel page need to be approved by the channel owner to be publicly visible. The default value is false.
            'Language':response['items'][i]['brandingSettings']['channel'].get('defaultLanguage','')
            }
        all_channels_data.append(data)

    return all_channels_data

# 3-Fetch Video IDs:
def get_video_ids(Playlist_ids):

    video_ids = []

    for Playlist_id in Playlist_ids:

        request = youtube.playlistItems().list(
            part='contentDetails',
            playlistId=Playlist_id,
            maxResults=50, )
        response = request.execute()

        for i in range(len(response['items'])):
            video_ids.append(response['items'][i]['contentDetails']['videoId'])

        next_page_token = response.get('nextPageToken')
        more_page = True

        while more_page:
            if next_page_token is None:
                more_page = False
            else:
                request = youtube.playlistItems().list(
                    part='contentDetails',
                    playlistId=Playlist_id,
                    maxResults=50,
                    pageToken=next_page_token)
                response = request.execute()
                for i in range(len(response['items'])):
                    video_ids.append(response['items'][i]['contentDetails']['videoId'])

                next_page_token = response.get('nextPageToken')

    return video_ids


# 4-Fetch_Video_ids-Final
def get_video_ids_final(video_ids):

    sorted_videos = []

    for i in range(0, len(video_ids), 50):
        request = youtube.videos().list(
            part='statistics,snippet',
            id=','.join(video_ids[i:i + 50]))

        response = request.execute()

        for video in response.get('items', []):
            video_final = {
                'Channel_ID': video['snippet']['channelId'],
                'view_count': int(video['statistics'].get('viewCount', 0)),
                'video_id': video['id']
            }
            sorted_videos.append(video_final)
    sorted_videos=pd.DataFrame(sorted_videos)
    sorted_video_ids = sorted_videos.sort_values(by=['Channel_ID', 'view_count'], ascending=[True, False])
    video_ids_final = sorted_video_ids.groupby('Channel_ID').head(100)

    return video_ids_final


# 5-Table_1:Avg
def get_average_duration(video_ids_final):

    channel_data = {}

    for i in range(0, len(video_ids_final), 50):
        video_response = youtube.videos().list(
            part='snippet,contentDetails',
            id=','.join(video_ids_final[i:i + 50])).execute()

        for video in video_response['items']:
            channel_id = video['snippet']['channelId']
            video_duration = video['contentDetails']['duration']
            duration_seconds = parse_duration(video_duration).total_seconds()

            if channel_id in channel_data:
                channel_data[channel_id]['total_duration'] += duration_seconds
                channel_data[channel_id]['video_count'] += 1
            else:
                channel_data[channel_id] = {
                    'total_duration': duration_seconds,
                    'video_count': 1
                }

    average_durations = []

    for channel_id, data in channel_data.items():
        total_duration = data['total_duration']
        video_count = data['video_count']
        average_duration = total_duration / video_count / 60.0
        average_duration_round = round(average_duration, 2)

        average_durations.append({
            'Channel_ID': channel_id,
            'Average_Duration': average_duration_round
        })

    return average_durations

# 6- Table_2:Videos
def get_all_vidoes_info(video_ids_final):

    all_videos_info = []

    for video_id in video_ids_final:
        # time.sleep(1)
        video_response=youtube.videos().list(
            part='snippet,statistics,contentDetails,status',
            id=video_id).execute()

        for item in video_response['items']:
            Length = item['contentDetails']['duration']
            duration = Length[2:]
            if len(duration)<5 and ':' not in duration:
                duration = '00:' + duration
            if len(duration)<5 and duration[-1]==':':
                duration = duration+'00'
            else:
                duration=duration
            duration = duration.replace('H', ':').replace('M', ':').replace('S', '')
            time_components = duration.split(':')
            formatted_duration = ':'.join(time_components)

            video_infos={
                    'Channel_ID': item['snippet']['channelId'],
                    'Video_ID': item['id'],
                    'Title': item['snippet']['title'],
                    'Description' :item['snippet']['description'],
                    'Upload_Date': item['snippet']['publishedAt'],
                    'Length': formatted_duration,
                    'Views': float(item['statistics'].get('viewCount',0)),
                    'Likes': float(item['statistics'].get('likeCount',0)) ,
                    'Comments':float(item['statistics'].get('commentCount', 0)),
                    'Tags': item['snippet'].get('tags', []),
                    'Upload_Status': item['status']['uploadStatus'],
                    'Privacy_Status': item['status']['privacyStatus'],
                    'Licensed_Content': item['contentDetails']['licensedContent'],
                    'Video_Language': item['snippet'].get('defaultAudioLanguage', []),
                    'Category_ID': item['snippet']['categoryId'],
                    'Video_Definition': item['contentDetails']['definition']
                    }

            category_response = youtube.videoCategories().list(
                part='snippet',
                id=item['snippet']['categoryId']
            ).execute()

            for category_item in category_response['items']:
                video_infos['Video_Category'] = category_item['snippet']['title']

                all_videos_info.append(video_infos)

    return all_videos_info


# 7-Table2:Comments
def get_comments_data(video_ids_final):

    all_comments_info = []

    for video_id in video_ids_final:
        # time.sleep(1)
        video_response = youtube.videos().list(
                    part='snippet',
                    id=video_id
                ).execute()

        channel_id = video_response['items'][0]['snippet']['channelId']

        try:
            response = youtube.commentThreads().list(
                part='snippet,id,replies',
                videoId=video_id,
                maxResults=100,
                order='time',
                    ).execute()
        except googleapiclient.http.HttpError as e:
            continue

        for item in response['items']:
            comment_snippet = item.get('snippet', {})
            topLevelComment = comment_snippet.get('topLevelComment', {})

            comment_data = {
                'Channel_ID': channel_id,
                'Comment_ID': topLevelComment.get('id', ''),
                'Parent_ID': item.get('id', ''),
                'Video_ID': comment_snippet['videoId'],
                'Comment': topLevelComment.get('snippet', {}).get('textDisplay', ''),
                'Comment_Date': topLevelComment.get('snippet', {}).get('publishedAt', ''),
                'Comment_Update_Date': topLevelComment.get('snippet', {}).get('updatedAt', ''),
                'Likes_Count': topLevelComment.get('snippet', {}).get('likeCount', ''),
                'Reply_Count': int(item['snippet']['totalReplyCount']),
                'author_Display_Name': topLevelComment.get('snippet', {}).get('authorDisplayName', ''),
                'Author_channel_ID': topLevelComment.get('snippet', {}).get('authorChannelId', {}).get('value', None),
                 }
            all_comments_info.append(comment_data)

    return all_comments_info


# 8-Tabel4: Replies
def get_replies_data(comment_IDs):

    all_replies_info = []

    half_length = len(comment_IDs) // 2
    api_key_1 = 'AIzaSyAX8Gm0i78R4cHvTkfyLc2MKgIlFjmDZg0' #final-2
    youtube_1 = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key_1)

    for comment_id in comment_IDs[:half_length]:

        replies_request = youtube_1.comments().list(
            part='snippet,id',
            parentId=comment_id,
            maxResults=20
        )
        replies_response = replies_request.execute()

        for reply_item in replies_response['items']:
            reply_data = {
                'Reply_id': reply_item['id'],
                'Comment_ID': comment_id,
                'Reply': reply_item['snippet']['textDisplay'],
                'Reply_date': reply_item['snippet']['publishedAt'],
                'Likes_of_reply': reply_item['snippet']['likeCount'],
            }

            all_replies_info.append(reply_data)

    api_key_2 = 'AIzaSyD-njrugMXooIoSlRlgNl5_PxtkW3b2I_4' #final-3
    youtube_2 = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key_2)

    for comment_id in comment_IDs[half_length:]:
        replies_request = youtube_2.comments().list(
            part='snippet,id',
            parentId=comment_id,
            maxResults=20
        )
        replies_response = replies_request.execute()

        for reply_item in replies_response['items']:
            reply_data = {
                'Reply_id': reply_item['id'],
                'Comment_ID': comment_id,
                'Reply': reply_item['snippet']['textDisplay'],
                'Reply_date': reply_item['snippet']['publishedAt'],
                'Likes_of_reply': reply_item['snippet']['likeCount'],
            }

            all_replies_info.append(reply_data)

    return all_replies_info



# Funtion1
channel_ids=get_channel_ids(channel_names)

# Function2
channel_info=pd.DataFrame(get_channel_info(channel_ids))
Playlist_ids = channel_info.iloc[:]['Playlist_ID'].tolist()

# Function3
video_ids=get_video_ids(Playlist_ids)

# Function4
video_ids_final=pd.DataFrame(get_video_ids_final(video_ids))
video_ids_final=video_ids_final['video_id'].tolist()

# Function6
video_data=pd.DataFrame(get_all_vidoes_info(video_ids_final))

# Function5
average_duration=pd.DataFrame(get_average_duration(video_ids_final))
Comments=pd.DataFrame(video_data.groupby('Channel_ID')['Comments'].agg('sum'))
likes=pd.DataFrame(video_data.groupby('Channel_ID')['Likes'].agg('sum'))
Channel_Data_1=pd.merge(channel_info,average_duration,on='Channel_ID')
channel_data_2=pd.merge(Channel_Data_1,Comments,on='Channel_ID')
Channel_Data=pd.merge(channel_data_2,likes,on='Channel_ID')

# Table1
Channel_Data.to_excel('1-Channel_Data.xlsx', index=False)

# Table2
video_data.to_excel('2-Videos.xlsx', index=False)

# Function7
comment_data=pd.DataFrame(get_comments_data(video_ids_final))

# Table3
comment_data.to_excel('Comments_data.xlsx')
comment_IDs = comment_data[comment_data['Reply_Count'] > 0]['Comment_ID'].tolist()

# Function8
replies_data=pd.DataFrame(get_replies_data(comment_IDs))

# Table4
replies_data.to_excel('Replies_data.xlsx')
comment_data['Comment_ID'] = comment_data['Comment_ID'].astype(str)
replies_data['Comment_ID'] = replies_data['Comment_ID'].astype(str)
comments_replies=pd.merge(comment_data,replies_data,on='Comment_ID',how='left')

# Tabel5
comments_replies.to_excel('Comments_Replies.xlsx', index=False)

end_time=time.time()
run_time=end_time-start_time
print(f'Run Time:{run_time} (sec)')
